# Attention-GAN for Image Colorization

This project implements an Attention-Generative Adversarial Network (GAN) for image colorization. The model takes a grayscale image as input and attempts to generate a realistic color version of it.

## Project Goal

The goal is to develop a deep learning model that can effectively colorize grayscale images, leveraging the power of GANs and attention mechanisms to produce visually appealing results.

## Model Architecture

The project utilizes two main components:

1.  **Generator:** A U-Net-like architecture with integrated attention blocks. The attention blocks help the generator focus on important regions of the image during colorization.
    *   **Encoder:** Downsamples the input grayscale image through convolutional and pooling layers.
    *   **Bottleneck:** A convolutional layer connecting the encoder and decoder.
    *   **Decoder:** Upsamples the feature maps and incorporates skip connections from the encoder. Attention blocks are placed in the decoder path to refine the upsampling process using information from the corresponding encoder layers.
    *   **Output:** A final convolutional layer generates the 3-channel color image.

2.  **Discriminator:** A simple Convolutional Neural Network that classifies whether an image is real (from the dataset) or fake (generated by the generator).

The model also incorporates a pre-trained VGG19 network to compute a **perceptual loss**, which helps the generator produce visually realistic and structurally similar color images.

## Loss Functions

The training process involves optimizing both the generator and the discriminator using a combination of loss functions:

*   **Discriminator Loss:** Binary cross-entropy loss is used to train the discriminator to distinguish between real and fake images.
*   **Generator Loss:**
    *   **GAN Loss:** Binary cross-entropy loss to fool the discriminator into thinking generated images are real.
    *   **L1 Loss:** Measures the pixel-wise difference between the generated color image and the ground truth color image.
    *   **Perceptual Loss:** Calculated as the L1 difference between the feature maps of the generated and real images extracted from an intermediate layer of the pre-trained VGG19 model.

## Training

The model is trained iteratively using the defined loss functions and optimizers. Early stopping based on the PSNR metric is implemented to prevent overfitting and save the best performing generator and discriminator models.

The training loop iterates through epochs, calculating and applying gradients for both the generator and discriminator. Evaluation metrics like PSNR and SSIM are calculated for each epoch to monitor the training progress.

## Data

The project uses the `imagenette/320px` dataset loaded using `tensorflow_datasets`. The dataset is preprocessed to resize images to 64x64 pixels and convert them to grayscale for the generator input, while keeping the original color images as ground truth.

## Dependencies

The following libraries are required to run the code:

*   `tensorflow`
*   `numpy`
*   `matplotlib`
*   `scikit-image` (for SSIM and PSNR calculations)
*   `tensorflow_datasets`

These dependencies are imported at the beginning of the notebook.

## Code Structure

The notebook is structured as follows:

1.  **Library Imports:** Imports all necessary libraries.
2.  **Data Loading and Preprocessing:** Loads the dataset and defines the preprocessing function.
3.  **Model Definitions:** Defines the attention block, generator, and discriminator architectures.
4.  **Perceptual Model Loading:** Loads the pre-trained VGG19 model for perceptual loss calculation.
5.  **Model Initialization and Optimization:** Initializes the generator and discriminator models, defines the loss function and optimizers.
6.  **Training Loop:** Implements the training process with early stopping and metric evaluation.
7.  **Result Visualization:** Displays example input, generated, and ground truth images after training.

## How to Use

1.  Run all the code cells in the notebook sequentially.
2.  The training process will start and periodically display generated images and training metrics.
3.  The best performing generator and discriminator models will be saved as `best_generator.keras` and `best_discriminator.keras` respectively.
4.  After training, the final comparison of input, generated, and ground truth images will be displayed.

## Further Improvements

*   Experiment with different GAN architectures and loss functions.
*   Train on a larger and more diverse dataset.
*   Implement more sophisticated attention mechanisms.
*   Incorporate other evaluation metrics.
*   Allow users to provide their own grayscale images for colorization.
